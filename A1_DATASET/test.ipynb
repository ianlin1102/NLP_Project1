{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85056002-8707-4663-9242-abe9dcec2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple, Callable\n",
    "from string import punctuation\n",
    "\n",
    "START_TOKEN: str = \"<s>\"\n",
    "END_TOKEN: str = \"</s>\"\n",
    "UNKNOWN_TOKEN: str = \"<UNK>\"\n",
    "\n",
    "def get_sanitized_n_gram(filename: str, counter_fn: Callable[[Counter, List[str]], None]) -> Counter:\n",
    "    result_counter: Counter = Counter()\n",
    "    with open(filename, \"r\") as file_object:\n",
    "        # File is already document sentence tokenized\n",
    "        for comment in file_object.readlines(): \n",
    "            # pad each comment with start..stop tokens\n",
    "            words = [START_TOKEN] \n",
    "            for unsanitized_word in comment.strip().split():\n",
    "                # keep a punctuation-free vocabulary\n",
    "                if unsanitized_word not in punctuation:\n",
    "                    words.append(unsanitized_word)\n",
    "            words.append(END_TOKEN)\n",
    "\n",
    "            # result_counter is passed in an out-param\n",
    "            counter_fn(result_counter, words)\n",
    "\n",
    "    return result_counter\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850f35f-b2e7-4db1-b445-f497ebd70564",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNIT TESTS ###\n",
    "\n",
    "def _counter_fn (ctr, words): \n",
    "    ctr += Counter(words)\n",
    "\n",
    "_test_words: Counter = get_sanitized_n_gram(\"train.txt\", _counter_fn)\n",
    "assert len(_test_words) > 0\n",
    "list(_test_words.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unigram(filename: str) -> Counter:\n",
    "    def _unigram_collector(unigram_counter: Counter, words: List[str]):\n",
    "        for word in words:\n",
    "            unigram_counter[word] += 1\n",
    "    return get_sanitized_n_gram(filename, _unigram_collector)\n",
    "\n",
    "def collect_bigram(filename: str) -> Counter:\n",
    "    def _bigram_collector(bigram_counter: Counter, words: List[str]):\n",
    "        # note that the last word can't form a bigram (OOB)\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i + 1]) \n",
    "            bigram_counter[bigram] += 1\n",
    "    \n",
    "    return get_sanitized_n_gram(filename, _bigram_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ee246-5661-4e45-aa5d-0a0146592f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram_counter: Counter = collect_unigram(\"train.txt\") \n",
    "train_bigram_counter: Counter = collect_bigram(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "  def __init__(self, unigram_counts: Counter, bigram_counts: Counter=None):\n",
    "    self.unigram_counts: Counter = unigram_counts # ( word, number_occurrence)\n",
    "    self.bigram_counts: Counter = bigram_counts\n",
    "    self.total_unigram_count: int = sum(self.unigram_counts.values())\n",
    "\n",
    "    if bigram_counts: # If asking for Bigram Model\n",
    "      self.bigram_context_counter: Counter = Counter()\n",
    "      for (word_1, word_2), count in bigram_counts.items():\n",
    "        self.bigram_context_counter[word_1] += count # This is the same as the unigarm_counts\n",
    "      \n",
    "\n",
    "  def unigram_probability(self, word: str) -> float:\n",
    "    return self.unigram_counts[word] / self.total_unigram_count\n",
    "  \n",
    "  def bigram_probability(self, ask: str, given: Tuple[str, str]) -> float:\n",
    "    if self.bigram_context_counter[given] == 0: \n",
    "      print(\"Unseen Before\")\n",
    "      return 0 # Unseen before\n",
    "    else:\n",
    "      # Return number of times { ask | Given} \\over number of {Given}\n",
    "      return self.bigram_counts[(given, ask)]/ self.bigram_context_counter[given]\n",
    "  \n",
    "  def add_k_smoothing(self, w1: str, w2: str | None=None, k: int=1) -> float: #Add 1 Method\n",
    "    v_size = len(self.unigram_counts)\n",
    "    if w2 is None:  # Unigram\n",
    "        return (self.unigram_counts[w1] + k) / (self.total_unigram_count + ( k * v_size ))\n",
    "    else:  # Bigram \n",
    "        return (self.bigram_counts[(w1, w2)] + k) / ( self.bigram_context_counter[w1] + (v_size  * k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f14ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = NGramModel(train_unigram_counter, train_bigram_counter)\n",
    "print(tester.unigram_counts['I'], tester.bigram_context_counter[\"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa724cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bi = collect_bigram(\"example.txt\")\n",
    "example_uni = collect_unigram(\"example.txt\")\n",
    "example_tester = NGramModel(example_uni, example_bi)\n",
    "print(example_bi)\n",
    "print(example_tester.bigram_context_counter)\n",
    "print(example_tester.unigram_counts)\n",
    "print(example_tester.unigram_probability(\"the\"), example_tester.unigram_probability(\"like\"))\n",
    "print(example_tester.bigram_probability(ask=\"the\", given=\"like\"), example_tester.bigram_probability(\"students\", \"the\")) # 1.0, 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc965baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_unknown_words(word_counts, bigram_counts, threshold=5):\n",
    "  # The idea is to remake the Tuple frequency: for any tuple such that frequency < 'threshold', replace with  <UNK>\n",
    "  vocab = set()\n",
    "  unknown = 0\n",
    "  special_token = {START_TOKEN, END_TOKEN, UNKNOWN_TOKEN}\n",
    "  vocab.update(special_token)\n",
    "\n",
    "  for word, count in word_counts.items():\n",
    "    if count > threshold or word in special_token: # In case the file has less than threshold tokens.\n",
    "      vocab.add(word)\n",
    "    else:\n",
    "      unknown += count\n",
    "    \n",
    "  #After the for loop, the total \"Unknown\" should be equal to SUM(vocabs < threshold)\n",
    "  new_counter = Counter()\n",
    "  new_counter[UNKNOWN_TOKEN] = unknown\n",
    "  for word, count in word_counts.items():\n",
    "    if word in vocab:\n",
    "      new_counter[word] = count\n",
    "  \n",
    "  processed_bigram = Counter()\n",
    "  \n",
    "  for (w1, w2), count in bigram_counts.items():\n",
    "    new_w1 = w1 if w1 in vocab else UNKNOWN_TOKEN\n",
    "    new_w2 = w2 if w2 in vocab else UNKNOWN_TOKEN\n",
    "    processed_bigram[(new_w1, new_w2)] += count\n",
    "\n",
    "  return vocab, new_counter, processed_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b541d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_file, threshold = 5) -> NGramModel:\n",
    "  # Collect frequency in the train.txt\n",
    "  train_unigram = collect_unigram(train_file) # ('word', frequency)\n",
    "  train_bigram = collect_bigram(train_file) # (('Given', 'Ask'), frequency)\n",
    "\n",
    "  vocab, processed_uni, processed_bi = handle_unknown_words(train_unigram, train_bigram, threshold) #Vocab with frequency > 5, and using the criteria to get processed_uni and bi, which contains <UNK>\n",
    "\n",
    "  model = NGramModel(processed_uni, processed_bi)\n",
    "  model.vocab = vocab\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbad6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "def perplexity(val_file: str, model: NGramModel, k_smoothing: int, verbose: bool= False) -> float:\n",
    "  pp: float = 0\n",
    "  val_file_words: int = 0\n",
    "  unknown_count: int = 0\n",
    "  \n",
    "  with open(val_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      line = line.strip()\n",
    "      if not line:\n",
    "        continue\n",
    "      \n",
    "      words: List[str] = [START_TOKEN]\n",
    "      words += line.split()\n",
    "      words.append(END_TOKEN)\n",
    "\n",
    "      words = [w if w in model.vocab else UNKNOWN_TOKEN for w in words] \n",
    "      # unknown_count += sum(1 for orig, new in zip ( original_words, words ) if new == '<UNK>')\n",
    "      for i in range(len(words)):\n",
    "        if words[i] in punctuation:\n",
    "            continue\n",
    "        if i == 0:\n",
    "          prob = model.add_k_smoothing(words[i], k = k_smoothing)\n",
    "        else:\n",
    "          prob = model.add_k_smoothing(words[i - 1], words[i], k_smoothing)\n",
    "        if prob == 0:\n",
    "          sys.stderr.write(f\"Token \\'{words[i]}\\' has 0 smoothed probability.\")\n",
    "        else:\n",
    "          pp += math.log(prob)\n",
    "          \n",
    "        val_file_words += 1\n",
    "          \n",
    "        if verbose:\n",
    "          print(f\"Unknown word \\'{words[i]}\\' in {val_file}: {unknown_count}\")\n",
    "    return math.exp( -pp / val_file_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "  model = train_model(\"train.txt\", threshold=5)\n",
    "\n",
    "  print(\"Vocab size :\", len( model.vocab  ))\n",
    "  print(\"Unigram\", model.total_unigram_count)\n",
    "\n",
    "  for k in [0.1, 0.6, 1, 2]:\n",
    "    train_pp: float = perplexity(\"train.txt\", model, k)\n",
    "    val_pp: float = perplexity(\"val.txt\", model, k)\n",
    "    print(f\"k= {k}: Train PP = {train_pp}, val pp = {val_pp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233873df",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
