{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0169c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.__version__)\n",
    "#print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "#print(\"MPS built:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Ground\n",
    "file_object = open( \"train.txt\", \"r\" )\n",
    "test_lines = file_object.readlines()[:2]\n",
    "result = []\n",
    "for comment in test_lines:\n",
    "  comment = comment.strip()\n",
    "  comment = \"<s> \" + comment + \" </s>\"\n",
    "  print(comment)\n",
    "    \n",
    "\n",
    "# print(test_line)\n",
    "# test_lines = sent_tokenize(test_line)\n",
    "# test = Counter()\n",
    "# print(test_lines)\n",
    "# for line in test_lines:\n",
    "#   line = \"<s> \" + line + \" </s>\"\n",
    "#   print(line)\n",
    "#   line = line.split()\n",
    "#   for word in line:\n",
    "#     test[word] += 1\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unigram(filename):\n",
    "    unigram_counter = Counter()\n",
    "    \n",
    "    with open(filename, \"r\") as file_object:\n",
    "        for comment in file_object.readlines(): # File is already document sentence tokenized\n",
    "            comment = comment.strip() # Remove white space beginning/end\n",
    "            comment = \" <s> \" + comment + \" </s> \" # Comment delimiter\n",
    "            #print(comment)\n",
    "            words = comment.split()  # Split into individual tokens\n",
    "            for word in words:\n",
    "                unigram_counter[word] += 1  # Count each word\n",
    "    \n",
    "    return unigram_counter\n",
    "\n",
    "def collect_bigram(filename):\n",
    "    bigram_counter = Counter()\n",
    "    \n",
    "    with open(filename, \"r\") as file_object:\n",
    "        for comment in file_object.readlines():\n",
    "            comment = comment.strip()\n",
    "            comment = \" <s> \" + comment + \" </s> \"\n",
    "            words = comment.split()\n",
    "            \n",
    "            for i in range(len(words) - 1): # Last one can't form a Tuple\n",
    "                bigram = (words[i], words[i + 1])  # Tuple of two consecutive words\n",
    "                bigram_counter[bigram] += 1\n",
    "    \n",
    "    return bigram_counter\n",
    "\n",
    "train_unigram_counter = collect_unigram(\"train.txt\") \n",
    "train_bigram_counter = collect_bigram(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Ground\n",
    "\n",
    "\n",
    "sum(train_unigram_counter.values())\n",
    "#print(unigram_counter['I'])\n",
    "print(train_bigram_counter)\n",
    "#for (w1, w2), count in bigram_counter.items():\n",
    "  #print(count)\n",
    "print(train_bigram_counter[('.', 'The')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "  def __init__(self, unigram_counts, bigram_counts=None):\n",
    "    self.unigram_counts = unigram_counts # ( word, number_occurrence)\n",
    "    self.bigram_counts = bigram_counts\n",
    "    self.total_unigram_count = sum(self.unigram_counts.values())\n",
    "\n",
    "    if bigram_counts: # If asking for Bigram Model\n",
    "      self.bigram_context_counter = Counter()\n",
    "      for (word_1, word_2), count in bigram_counts.items(): # item() format is Tuple(Tuple(word_1, word_2), number_appearance)\n",
    "        self.bigram_context_counter[word_1] += count # This is the same as the unigarm_counts\n",
    "      \n",
    "\n",
    "  def unigram_probability(self, word):\n",
    "    return self.unigram_counts[word] / self.total_unigram_count\n",
    "  \n",
    "  def bigram_probability(self, ask, given):\n",
    "    if self.bigram_context_counter[given] == 0: \n",
    "      print(\"Unseen Before\")\n",
    "      return 0 # Unseen before\n",
    "    else:\n",
    "      # Return number of times { ask | Given} \\over number of {Given}\n",
    "      return self.bigram_counts[(given, ask)]/ self.bigram_context_counter[given]\n",
    "  \n",
    "  def add_k_smoothing(self, w1, w2=None, k = 1): #Add 1 Method\n",
    "    v_size = len(self.unigram_counts)\n",
    "    if w2 is None:  # Unigram\n",
    "        return (self.unigram_counts[w1] + k) / (self.total_unigram_count + ( k * v_size ))\n",
    "    else:  # Bigram \n",
    "        return (self.bigram_counts[(w1, w2)] + k) / ( self.bigram_context_counter[w1] + (v_size  * k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f14ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = NGramModel(train_unigram_counter, train_bigram_counter)\n",
    "print(tester.unigram_counts['I'], tester.bigram_context_counter[\"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa724cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bi = collect_bigram(\"example.txt\")\n",
    "example_uni = collect_unigram(\"example.txt\")\n",
    "example_tester = NGramModel(example_uni, example_bi)\n",
    "print(example_bi)\n",
    "print(example_tester.bigram_context_counter) #\n",
    "print(example_tester.unigram_counts)\n",
    "print(example_tester.unigram_probability(\"the\"), example_tester.unigram_probability(\"like\"))\n",
    "print(example_tester.bigram_probability(ask=\"the\", given=\"like\"), example_tester.bigram_probability(\"students\", \"the\")) # 1.0, 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc965baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_unknown_words(word_counts, bigram_counts, threshold=5):\n",
    "  # The idea is to remake the Tuple frequency: for any tuple such that frequency < 'threshold', replace with  <UNK>\n",
    "  vocab = set()\n",
    "  unknown = 0\n",
    "  special_token = {'<s>', '</s>', '<UNK>'}\n",
    "  vocab.update(special_token)\n",
    "\n",
    "  for word, count in word_counts.items():\n",
    "    if count > threshold or word in special_token: # In case the file has less than threshold tokens.\n",
    "      vocab.add(word)\n",
    "    else:\n",
    "      unknown += count\n",
    "    \n",
    "  #After the for loop, the total \"Unknown\" should be equal to SUM(vocabs < threshold)\n",
    "  new_counter = Counter()\n",
    "  new_counter[\"<UNK>\"] = unknown\n",
    "  for word, count in word_counts.items():\n",
    "    if word in vocab:\n",
    "      new_counter[word] = count\n",
    "  \n",
    "  processed_bigram = Counter()\n",
    "  \n",
    "  for (w1, w2), count in bigram_counts.items():\n",
    "    new_w1 = w1 if w1 in vocab else '<UNK>'\n",
    "    new_w2 = w2 if w2 in vocab else '<UNK>'\n",
    "    processed_bigram[(new_w1, new_w2)] += count\n",
    "\n",
    "  return vocab, new_counter, processed_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b541d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_file, threshold = 5):\n",
    "  # Collect frequency in the train.txt\n",
    "  train_unigram = collect_unigram(train_file) # ('word', frequency)\n",
    "  train_bigram = collect_bigram(train_file) # (('Given', 'Ask'), frequency)\n",
    "\n",
    "  vocab, processed_uni, processed_bi = handle_unknown_words(train_unigram, train_bigram, threshold) #Vocab with frequency > 5, and using the criteria to get processed_uni and bi, which contains <UNK>\n",
    "\n",
    "  model = NGramModel(processed_uni, processed_bi)\n",
    "  model.vocab = vocab\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbad6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(val_file, model: NGramModel, k_smoothing):\n",
    "  pp = 0\n",
    "  val_file_words = 0\n",
    "  unknown_count = 0\n",
    "  \n",
    "  with open(val_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      line = line.strip()\n",
    "      if not line:\n",
    "        continue\n",
    "      line = \" <s> \" + line + \" </s> \"\n",
    "      words = line.split()\n",
    "\n",
    "      original_words = words.copy()\n",
    "      words = [w if w in model.vocab else '<UNK>' for w in words] \n",
    "      # unknown_count += sum(1 for orig, new in zip ( original_words, words ) if new == '<UNK>')\n",
    "      for i in range(len(words)):\n",
    "        if i == 0:\n",
    "          prob = model.add_k_smoothing(words[i],k = k_smoothing)\n",
    "        else:\n",
    "          prob = model.add_k_smoothing(words[i - 1], words[i], k_smoothing)\n",
    "        if prob == 0:\n",
    "          print(\"Uh, Why 0 prob?\")\n",
    "        else:\n",
    "          pp += math.log(prob)\n",
    "          \n",
    "        val_file_words += 1\n",
    "        print(f\"Unknown word in {val_file}: {unknown_count}\")\n",
    "    return math.exp( -pp / val_file_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fff1e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  model = train_model(\"train.txt\", threshold=5)\n",
    "\n",
    "  print(\"Vocab size :\", len( model.vocab  ))\n",
    "  print(\"Unigram\", model.total_unigram_count)\n",
    "\n",
    "  for k in [0.1, 0.6, 1, 2]:\n",
    "    train_pp = perplexity(\"train.txt\", model, k)\n",
    "    val_pp = perplexity(\"val.txt\", model, k)\n",
    "    print(f\"k= {k}: Train PP = {train_pp}, val pp = {val_pp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "233873df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 1387\n",
      "Unigram 90708\n",
      "k= 0.1: Train PP = 58.192887650092786, val pp = 89.73473766774906\n",
      "k= 0.6: Train PP = 118.21592974102361, val pp = 142.1014238772904\n",
      "k= 1: Train PP = 150.03057788578084, val pp = 170.60179494345215\n",
      "k= 2: Train PP = 209.12321470217225, val pp = 224.03332629370527\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
