{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0169c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.__version__)\n",
    "#print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "#print(\"MPS built:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Ground\n",
    "file_object = open( \"train.txt\", \"r\" )\n",
    "test_lines = file_object.readlines()[:2]\n",
    "result = []\n",
    "for comment in test_lines:\n",
    "  comment = comment.strip()\n",
    "  comment = \"<s> \" + comment + \" </s>\"\n",
    "  print(comment)\n",
    "    \n",
    "\n",
    "# print(test_line)\n",
    "# test_lines = sent_tokenize(test_line)\n",
    "# test = Counter()\n",
    "# print(test_lines)\n",
    "# for line in test_lines:\n",
    "#   line = \"<s> \" + line + \" </s>\"\n",
    "#   print(line)\n",
    "#   line = line.split()\n",
    "#   for word in line:\n",
    "#     test[word] += 1\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unigram(filename):\n",
    "    unigram_counter = Counter()\n",
    "    \n",
    "    with open(filename, \"r\") as file_object:\n",
    "        for comment in file_object.readlines(): # File is already document sentence tokenized\n",
    "            comment = comment.strip() # Remove white space beginning/end\n",
    "            comment = \" <s> \" + comment + \" </s> \" # Comment delimiter\n",
    "            #print(comment)\n",
    "            words = comment.split()  # Split into individual tokens\n",
    "            for word in words:\n",
    "                unigram_counter[word] += 1  # Count each word\n",
    "    \n",
    "    return unigram_counter\n",
    "\n",
    "def collect_bigram(filename):\n",
    "    bigram_counter = Counter()\n",
    "    \n",
    "    with open(filename, \"r\") as file_object:\n",
    "        for comment in file_object.readlines():\n",
    "            comment = comment.strip()\n",
    "            comment = \" <s> \" + comment + \" </s> \"\n",
    "            words = comment.split()\n",
    "            \n",
    "            for i in range(len(words) - 1): # Last one can't form a Tuple\n",
    "                bigram = (words[i], words[i + 1])  # Tuple of two consecutive words\n",
    "                bigram_counter[bigram] += 1\n",
    "    \n",
    "    return bigram_counter\n",
    "\n",
    "train_unigram_counter = collect_unigram(\"train.txt\") \n",
    "train_bigram_counter = collect_bigram(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Ground\n",
    "\n",
    "\n",
    "sum(train_unigram_counter.values())\n",
    "#print(unigram_counter['I'])\n",
    "print(train_bigram_counter)\n",
    "#for (w1, w2), count in bigram_counter.items():\n",
    "  #print(count)\n",
    "print(train_bigram_counter[('.', 'The')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "  def __init__(self, unigram_counts, bigram_counts=None):\n",
    "    self.unigram_counts = unigram_counts # ( word, number_occurrence)\n",
    "    self.bigram_counts = bigram_counts\n",
    "    self.total_unigram_count = sum(self.unigram_counts.values())\n",
    "\n",
    "    if bigram_counts: # If asking for Bigram Model\n",
    "      self.bigram_context_counter = Counter()\n",
    "      for (word_1, word_2), count in bigram_counts.items(): # item() format is Tuple(Tuple(word_1, word_2), number_appearance)\n",
    "        self.bigram_context_counter[word_1] += count # This is the same as the unigarm_counts\n",
    "      \n",
    "\n",
    "  def unigram_probability(self, word):\n",
    "    return self.unigram_counts[word] / self.total_unigram_count\n",
    "  \n",
    "  def bigram_probability(self, ask, given):\n",
    "    if self.bigram_context_counter[given] == 0: \n",
    "      print(\"Unseen Before\")\n",
    "      return 0 # Unseen before\n",
    "    else:\n",
    "      # Return number of times { ask | Given} \\over number of {Given}\n",
    "      return self.bigram_counts[(given, ask)]/ self.bigram_context_counter[given]\n",
    "  \n",
    "  def laplace_smoothing(self, w1, w2=None): #Add 1 Method\n",
    "      v_size = len(self.unigram_counts)\n",
    "      if w2 is None:  # Unigram\n",
    "          return (self.unigram_counts[w1] + 1) / (self.total_unigram_count + v_size)\n",
    "      else:  # Bigram \n",
    "          return (self.bigram_counts[(w1, w2)] + 1) / (self.bigram_context_counter[w1] + v_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f14ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = NGramModel(train_unigram_counter, train_bigram_counter)\n",
    "print(tester.unigram_counts['I'], tester.bigram_context_counter[\"I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa724cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bi = collect_bigram(\"example.txt\")\n",
    "example_uni = collect_unigram(\"example.txt\")\n",
    "example_tester = NGramModel(example_uni, example_bi)\n",
    "print(example_bi)\n",
    "print(example_tester.bigram_context_counter) #\n",
    "print(example_tester.unigram_counts)\n",
    "print(example_tester.unigram_probability(\"the\"), example_tester.unigram_probability(\"like\"))\n",
    "print(example_tester.bigram_probability(ask=\"the\", given=\"like\"), example_tester.bigram_probability(\"students\", \"the\")) # 1.0, 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc965baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_unknown_words(word_counts, bigram_counts, threshold=5):\n",
    "  # The idea is to remake the Tuple frequency: for any tuple such that frequency < 'threshold', replace with  <UNK>\n",
    "  vocab = set()\n",
    "  unknown = 0\n",
    "  special_token: {'<s>', '</s>', '<UNK>'}\n",
    "  vocab.update(special_token)\n",
    "\n",
    "  for word, count in word_counts.items():\n",
    "    if count > threshold or word in special_token: # In case the file has less than threshold tokens.\n",
    "      vocab.add(word)\n",
    "    else:\n",
    "      unknown += count\n",
    "    \n",
    "  #After the for loop, the total \"Unknown\" should be equal to SUM(vocabs < threshold)\n",
    "  new_counter = Counter()\n",
    "  new_counter[\"<UNK>\"] = unknown\n",
    "  for word, count in word_counts.items():\n",
    "    if word in vocab:\n",
    "      new_counter[word] = count\n",
    "  \n",
    "  processed_bigram = Counter()\n",
    "  \n",
    "  for (w1, w2), count in bigram_counts.items():\n",
    "    new_w1 = w1 if w1 in vocab else '<UNK>'\n",
    "    new_w2 = w2 if w2 in vocab else '<UNK>'\n",
    "    processed_bigram[(new_w1, new_w2)] += count\n",
    "\n",
    "  return vocab, new_counter, processed_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b541d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_file, threshold = 5):\n",
    "  # Collect frequency in the train.txt\n",
    "  train_unigram = collect_unigram(train_file) # ('word', frequency)\n",
    "  train_bigram = collect_bigram(train_file) # (('Given', 'Ask'), frequency)\n",
    "\n",
    "  vocab, processed_uni, processed_bi = handle_unknown_words(train_unigram, train_bigram, threshold) #\n",
    "\n",
    "  model = NGramModel(processed_uni, processed_bi)\n",
    "  model.vocab = vocab\n",
    "\n",
    "  return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
